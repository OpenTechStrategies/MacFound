#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file
#
# This is necessary because not everything can be addressed with the
# 'sanitize' script.  The 'sanitize' script isn't aware of CSV rows or
# columns; it just treats the whole CSV file as a normal text file.
# But for, e.g., fixing the YouTube links (see features.org for more),
# we really need a one-time transformation that knows what cell it's
# operating on.
#
# Copyright (C) 2017 Open Tech Strategies, LLC
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

##########################################################################
#                                                                        #
#   NOTE: This code is highly specific to the needs of the MacArthur     #
#   Foundation and is unlikely to be correct for your CSV.  It is        #
#   open source software, so please modify it to suit your needs.        #
#                                                                        #
##########################################################################

__doc__ = """\
Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file.

Usage:

  $ fix-csv CSV_INPUT FIXED_CSV_OUTPUT

There are no options, because this is for a one-time transformation;
anything that would be an option should just be hardcoded in anyway.
"""

import csv
import getopt
import re
import io
import os
import json
import pickle
import sys
import warnings
import string
from bs4 import BeautifulSoup


def collapse_replace(string, old, new):
    "Return STRING, with OLD repeatedly replaced by NEW until no more OLD."
    while string.find(old) != -1:
        string = string.replace(old, new)
    return string


def weaken_the_strong(html):
    """Strip any meaningless <strong>...</strong> tags from HTML.
    HTML is a Unicode string; the return value is either HTML or a new
    Unicode string based on HTML.

    If <strong> tags cover everything in HTML, remove the tags.  But if
    <strong> tags are used only sometimes, maybe they're meaningful, so
    leave them.  Basically, we want to make strength great again."""
    # If there's no strength here, move on.
    if not "<strong>" in html:
        return html

    # Remove the stuff inside strong tags
    soup = BeautifulSoup(html, "html.parser")
    while "<strong>" in str(soup):
        soup.strong.extract()

    # Check whether the non-bold stuff is more than just tags and
    # punctuation.  If not, all the important stuff was bold, so strip
    # bold and return.
    if re.sub(r"\W", "", soup.get_text()) == "":
        return re.sub("</?strong>", "", html)

    # OTOH, if the non-bold stuff contained letters or numbers, maybe
    # there's real content there, which means the html was a mix of
    # bold and non-bold text.  Better to leave it alone.
    return html


def form_well(html):
    """Return a well-formed version of HTML with dangling tags closed.
    Some of the input includes tables that cut off without closing
    tags; some entries leave <td> tags open too."""
    # Parse html and suppress warnings about urls in the text
    warnings.filterwarnings("ignore", category=UserWarning, module="bs4")
    soup = BeautifulSoup(html, "html.parser")

    # Return well-formed html as a soup object
    return soup


def fix_csv(
    fname_in,
    fname_out=None,
    pare=None,
    show_table_problem=False,
    reclassifications_csv=None,
):
    """Write a sanitized version of FNAME_IN (a CSV file) to FNAME_OUT.

    FNAME_IN is a filename.  FNAME_OUT is either a filename or None.
    If it is None, then write output to a returned StringIO object;
    otherwise, write output to the named file (and the return value is
    undefined).

    If PARE is not None, it is a positive integer indicating that only
    1 of every PARE entries should be processed, and the others skipped.

    If SHOW_TABLE_PROBLEM is not False, write debugging statements to
    sys.stderr to show entries that have unclosed <table> tags.

    If RECLASSIFICATIONS_CSV is not None, it is the path to a CSV file
    that contains reclassification information in column D (4) and an
    organization name in column A (1)."""
    reclass_csv_reader = None
    try:
        csv_reader = csv.reader(
            open(fname_in, encoding="utf-8"), delimiter=",", quotechar='"'
        )
        if reclassifications_csv is not None:
            reclass_csv_reader = csv.reader(
                open(reclassifications_csv, encoding="utf-8"),
                delimiter=",",
                quotechar='"',
            )
    except UnicodeDecodeError:
        sys.stderr.write(
            "fix-csv expects utf-8-encoded unicode, not whatever is in this csv file.\n"
        )
        sys.exit(-1)

    if fname_out:
        out = open(fname_out, "w", encoding="utf-8")
    else:
        out = io.StringIO()
    csv_writer = csv.writer(out, delimiter=",", quotechar='"', lineterminator="\n")

    header_row = next(csv_reader)
    csv_writer.writerow(header_row)

    # Keys are organization names; values are tuples of this form:
    #
    #   (original_classification, new_classification)
    #
    # In almost all cases, original_classification is "Other", but
    # there is at least one case where it's "Housing and Homelessness"
    # (the organization in that case is "WikiHouse Foundation").
    #
    # Sometimes instead of a the tuple, the value is None.  That means
    # this org name appeared more than once, so we can't use it
    # (because it's ambiguous -- we don't know which proposals it
    # applies to).  Our way of remembering that is to store None as
    # its value.
    reclassifications = {}
    if reclass_csv_reader is not None:
        ignored_headers = next(reclass_csv_reader)
        for row in reclass_csv_reader:
            if row[0] in reclassifications:
                reclassifications[row[0]] = None
            else:
                reclassifications[row[0]] = (
                    row[1],
                    row[3],
                )

    # Convert "<foo>&nbsp;<bar>" and "</foo>&nbsp;<bar>" to
    # to "<foo> <bar>" and "</foo> <bar>" respectively.  This is
    # because those particular instances of "&nbsp;" in the data are
    # not very convincing, and they create noise when we're looking
    # for unnecessary escaping elsewhere.
    intertag_nbsp_re = re.compile("(?m)(</?[a-z]+>)&nbsp;(<[a-z]+>)")

    # Actually, some invalid identifiers would still match this,
    # because this regular expression doesn't check the length.
    # That's okay; we check length manually at the call site.
    youtube_id_re = re.compile("^[-_a-zA-Z0-9]+$")

    bullets_re = re.compile("^â€¢", re.MULTILINE)

    attachments_to_upload = {}

    row_num = 0
    new_rows = []
    for row in csv_reader:
        row_num += 1  # first row here is row 1 (the header row was row 0)
        if pare is not None and row_num % pare != 0:
            continue

        if row[0] in reclassifications:
            if reclassifications[row[0]] is not None:
                old, new = reclassifications[row[0]]
                # Topic ("Primary Thematic Area" is Column 12; we call it
                # 11 here because row is 0-based not 1-based.
                found_old_class = row[11].strip().lower()
                old = old.strip().lower()
                # Mostly old should be "other", except in one case:
                # for organization "WikiHouse Foundation", old has
                # a value of 'Housing and Homelessness'.  And it's
                # the only one matched, which makes me think that
                # maybe the reclassification sheet's claim that
                # "other" appears in the original spreadsheet is
                # bogus, and those proposals actually have some
                # random different classification.
                if (
                    found_old_class == old
                    or (found_old_class == "select" and old == "other")
                    or (found_old_class == "" and old == "other")
                ):
                    # NOTE: This is not absolutely guaranteed to be
                    # correct.  It might be that a given org submitted
                    # multiple proposals, and that "Select" or "Other"
                    # is the value of its Topic column in more than
                    # one of those proposals, and that the MacFound
                    # staff reclassifier only meant to reclassify one
                    # of those proposals.  I suppose we could see if
                    # the same org gets reclassified multiple times,
                    # but want to ask Jeff about this first.
                    row[11] = new
        new_row = []
        org_name = None  # used only for debugging output
        cell_num = 0
        for cell in row:
            new_cell = cell
            # A straight-up HTML-unescaping might be the right thing
            # (i.e., new_cell = html_parser.unescape(new_cell) below)
            # in the long run, but for now, let's do the same limited
            # set of unescapings the original 'sanitize' script did:
            new_cell = new_cell.replace("&amp;", "&")
            new_cell = new_cell.replace("&lt;", "<")
            new_cell = new_cell.replace("&gt;", ">")
            # The rest should be recursively collapsing replacements:
            new_cell = collapse_replace(new_cell, "&nbsp;&nbsp;", "&nbsp;")
            new_cell = collapse_replace(new_cell, "&nbsp; ", " ")
            new_cell = collapse_replace(new_cell, " &nbsp;", " ")
            new_cell = collapse_replace(new_cell, "&nbsp;</", "</")
            new_cell = collapse_replace(new_cell, '\\"', '"')
            new_cell = re.sub(intertag_nbsp_re, "\\1 \\2", new_cell)
            if org_name is None:
                org_name = new_cell
            soup = form_well(new_cell)
            new_cell = weaken_the_strong(str(soup))

            # The parsing for lists requires an asterisk at the start
            # of the line, but some entries use bullets. This regex
            # swaps the bullets for asterisks.
            new_cell = bullets_re.sub("*", new_cell)
            new_cell = new_cell.strip()

            # Remember, the cell_nums referenced below are 0-based
            # *and* from 100andchangeExport-all-judges.csv before it
            # has any of the supplementary columns cut or spliced in.

            if cell_num == 11:  # Some entries have "Select" as the topic
                # Column 12 (11 in 0-based) is "Primary Thematic Area",
                # and it's usually "Affordable and Clean Energy" or
                # "Sustainable Cities, Communities and Regions" or
                # something like that.  But in a few entries it's
                # blank or "Select", which must be handled specially.
                new_cell = new_cell.strip()
                # The data has spurious case variants of the same
                # category, e.g., "Health and Well-Being" vs
                # "Health and Well-being".  Fortunately, we can fix it
                # with a very simple rule: capitalize every word,
                # except for "and" and "and/or", which we will make
                # sure are always lower-case.
                new_cell = string.capwords(new_cell)
                new_cell = new_cell.replace(" And ", " and ")
                new_cell = new_cell.replace(" And/or ", " and/or ")
                # string.capwords() doesn't treat a word immediately
                # following a hyphen as a separate word, so we need
                # to special-case the two known instances of that.
                new_cell = new_cell.replace("Well-being", "Well-Being")
                new_cell = new_cell.replace("Non-communicable", "Non-Communicable")
                # Often a comma was used to delimit the common prefix
                # "Health and Well-Being", but sometimes a colon was
                # used.  The colon probably makes more sense, so let's
                # just switch them all to that.
                new_cell = new_cell.replace(
                    "Health and Well-Being,", "Health and Well-Being:"
                )
                # And a couple of judgement calls:
                if re.match("^Resilience Against Climate Change$", new_cell):
                    # The shorter name only had 18 members, compared
                    # with 170 for the longer name.
                    new_cell = (
                        "Resilience Against Climate Change and Environmental Damage"
                    )
                if re.match("^Crime and Punishment$", new_cell):
                    # The shorter name only had 9 members, compared
                    # with 54 for the longer name.
                    new_cell = "Crime, Punishment and Rehabilitation"
                # Is it still a special case when the whole program is
                # special cases?
                if new_cell == "" or new_cell.lower() == "select":
                    # Cannot use special formatting here because this
                    # will be used as a category, not just as a value.
                    new_cell = "No Primary Thematic Area Selected"
            if (
                show_table_problem
                and new_cell.find("<table") != -1
                and new_cell.find("</table") == -1
            ):
                sys.stderr.write(
                    "DEBUG: unclosed <table>: row %d, col %d (%s): '%s':\n'%s'\n\n"
                    % (row_num, cell_num, header_row[cell_num - 1], org_name, new_cell)
                )
            if cell_num == 89:  # The pitch video cell may need fixing.
                # They all contain just an ID like "iIrGUi95ko4", not a URL
                # like "https://www.youtube.com/watch?v=iIrGUi95ko4".
                #
                # The rules for YouTube video identifiers seem to be:
                #
                #   * Exactly 11 characters long
                #   * Alphanumerics, "-", and "_" only (no spaces)
                #
                # There are a few special cases we handle below too.

                # "5m_6jsAwYNA " had trailing whitespace:
                new_cell = new_cell.strip()
                # "/JO7Eg6Kc-qk" kept their leading slash:
                if new_cell.find("/") == 0:
                    new_cell = new_cell[1:]
                if (
                    (len(new_cell) == 11 and youtube_id_re.match(new_cell))
                    # One valid ID had "&feature" tacked on to the
                    # end, and two had time markers in the URL.
                    # Since we know they're valid, just say yes.
                    or new_cell.find("&feature") != -1
                    or new_cell.find("?t=") != -1
                ):
                    # Okay, any of the above can become a video URL:
                    video_id = new_cell
                    # Note that the "#ev" embed code below requires
                    # the EmbedVideo MediaWiki extension.  See issue
                    # #26 for details.
                    new_cell = (
                        "https://www.youtube.com/watch?v="
                        + video_id
                        + "\n\n"
                        + "{{#ev:youtube|"
                        + video_id
                        + "}}"
                    )
                else:  # There were 8 with no video that we could find:
                    new_cell = (
                        "<span style="
                        + '"color: red; font-family: monospace;" >'
                        + 'Could not convert "'
                        + new_cell
                        + '" to a YouTube video URL.'
                        + "</span>"
                    )
            new_row.append(new_cell)

            cell_num += 1
        csv_writer.writerow(new_row)

    return out


def main():
    """Sanitize the MacFound input and emit it as html-ized csv."""
    try:
        opts, args = getopt.getopt(
            sys.argv[1:], "", ["pare=", "reclassifications=", "show-table-problem"]
        )
    except getopt.GetoptError as err:
        sys.stderr.write("ERROR: '%s'\n" % err)
        sys.exit(2)

    show_table_problem = False
    reclassifications_csv = None
    pare = None
    for o, a in opts:
        if o in ("--show-table-problem",):
            show_table_problem = True
        elif o in ("--reclassifications",):
            reclassifications_csv = a
        elif o == "--pare":
            pare = int(a)
        else:
            sys.stderr.write("ERROR: unrecognized option '%s'\n" % o)
            sys.exit(2)

    if len(args) != 2:
        sys.stderr.write("ERROR: need CSV_INPUT and NEW_CSV_OUTPUT arguments.\n\n")
        sys.stderr.write(__doc__)
        sys.exit(1)
    fix_csv(
        args[0],
        args[1],
        pare,
        show_table_problem,
        reclassifications_csv=reclassifications_csv,
    )


if __name__ == "__main__":
    main()
